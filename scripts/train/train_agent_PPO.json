{
    "id": "PPOExperimentExample",
    "environment": "Eplus-small_office-hot-discrete-stochastic-v1",
    "episodes": 10,
    "algorithm": {
        "name": "SB3-PPO",
        "log_interval": 1,
        "parameters": {
            "policy": "MlpPolicy",
            "learning_rate": 0.001,
            "n_steps": 4096,
            "batch_size": 128,
            "n_epochs": 15,
            "gamma": 0.9,
            "gae_lambda": 0.9,
            "clip_range": 0.2,
            "clip_range_vf": null,
            "normalize_advantage": true,
            "ent_coef": 0,
            "vf_coef": 0.5,
            "max_grad_norm": 0.5,
            "use_sde": false,
            "sde_sample_freq": -1,
            "rollout_buffer_class": null,
            "rollout_buffer_kwargs": null,
            "target_kl": null,
            "stats_window_size": 100,
            "tensorboard_log": null,
            "policy_kwargs": null,
            "verbose": 1,
            "seed": 3,
            "device": "auto",
            "_init_setup_model": true
        }
    },
    "env_params": {
        "reward": "LinearReward"
    },
    "seed": 3,
    "model": null,
    "wrappers": {
        "NormalizeAction": {},
        "NormalizeObservation": {},
        "LoggerWrapper": {
            "logger_class": "sinergym.utils.logger.CSVLogger",
            "flag": true
        }
    },
    "evaluation": {
        "eval_freq": 2,
        "eval_length": 2
    },
    "wandb": {
        "init_params": {
            "project": "sinergym",
            "entity": "huron-yin"
        },
        "artifact_name": "experiment_PPO",
        "artifact_type": "training",
        "dump_frequency": 1000
    }
}